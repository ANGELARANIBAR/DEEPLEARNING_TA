{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tA5CMeCI_ZMF"
      },
      "outputs": [],
      "source": [
        "#@title Importamos el Dataset { run: \"auto\" }\n",
        "!kaggle datasets download -d manideep1108/tusimple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip tusimple.zip"
      ],
      "metadata": {
        "id": "3ki8oyXi1iJG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image\n",
        "import keras\n",
        "from keras.preprocessing import image\n",
        "from keras.utils import plot_model\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import glob\n",
        "from tqdm.notebook import tqdm\n",
        "import json\n",
        "import cv2"
      ],
      "metadata": {
        "id": "5dukl9tt1oSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset{ run: \"auto\" }\n",
        "filenames = np.array(glob.glob('TUSimple/train_set/**/*.jpg', recursive=True))"
      ],
      "metadata": {
        "id": "6PFb7RdX1ist"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# Ruta a las imágenes\n",
        "image_path = './TUSimple/train_set'\n",
        "\n",
        "# Obtener la lista de archivos JSON de anotaciones\n",
        "annotation_paths = glob.glob(os.path.join(image_path, 'label_data_*.json'))\n",
        "print(\"Archivos JSON de anotaciones encontrados:\", annotation_paths)\n",
        "\n",
        "# Lista para almacenar los nombres de las imágenes del JSON\n",
        "json_image_names = []\n",
        "\n",
        "# Cargar nombres de imágenes desde todos los archivos JSON\n",
        "for annotation_path in annotation_paths:\n",
        "    with open(annotation_path, 'r') as f:\n",
        "        try:\n",
        "            # Leer todas las líneas del archivo\n",
        "            lines = f.readlines()\n",
        "\n",
        "            # Iterar sobre cada línea y cargar el JSON\n",
        "            for line in lines:\n",
        "                try:\n",
        "                    data = json.loads(line)\n",
        "                    # Suponiendo que hay una clave específica en cada JSON que contiene los nombres de las imágenes\n",
        "                    image_name = os.path.basename(data['raw_file'])\n",
        "                    json_image_names.append(image_name)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Error al decodificar JSON en {annotation_path}: {e}\")\n",
        "\n",
        "        except Exception as ex:\n",
        "            print(f\"Error al procesar {annotation_path}: {ex}\")\n",
        "\n",
        "print(\"Total de nombres de imágenes en JSON:\", len(json_image_names))\n",
        "\n",
        "# Patrón para buscar imágenes solo en las carpetas deseadas dentro de 'image_path'\n",
        "image_folders = ['clips/0531', 'clips/0601']  # Agrega los nombres de las carpetas deseadas aquí\n",
        "\n",
        "image_files = []\n",
        "for folder in image_folders:\n",
        "    folder_pattern = os.path.join(image_path, folder, '**', '*.jpg')\n",
        "    image_files.extend(glob.glob(folder_pattern, recursive=True))\n",
        "\n",
        "# Filtrar las imágenes reales para incluir solo las que están en json_image_names\n",
        "img_files = [img_path for img_path in image_files if os.path.basename(img_path) in json_image_names]\n",
        "\n",
        "print(\"\\nTotal de imágenes encontradas en los JSON:\", len(img_files))\n",
        "print(\"Ejemplo de imágenes encontradas:\")\n",
        "for img_path in img_files[:5]:  # Mostrar los primeros 5 ejemplos\n",
        "    print(img_path)\n"
      ],
      "metadata": {
        "id": "njB2cn6rcrt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "annotations = []\n",
        "for annotation_path in annotation_paths:\n",
        "    with open(annotation_path, 'r') as f:\n",
        "        for line in f:\n",
        "            annotations.append(json.loads(line))"
      ],
      "metadata": {
        "id": "Om7Pm58Acv1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Preparamos el dataset { run: \"auto\" }\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import cv2\n",
        "\n",
        "class LaneDataset(Sequence):\n",
        "    def __init__(self, image_files, annotations, batch_size=32, image_size=(512, 256)):\n",
        "        self.image_files = image_files\n",
        "        self.annotations = annotations\n",
        "        self.batch_size = batch_size\n",
        "        self.image_size = image_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_files) / self.batch_size))\n",
        "\n",
        "    def preprocess_image(self, image_path):\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.resize(image, self.image_size)\n",
        "        image = image / 255.0  # Normalizar la imagen\n",
        "        return image\n",
        "\n",
        "    def extract_lane_points(self, annotation):\n",
        "        # Implementa esta función según cómo estén estructuradas tus anotaciones\n",
        "        return annotation['lanes']\n",
        "\n",
        "    def pad_lanes(self, lanes, max_points=100):\n",
        "        padded_lanes = np.zeros((len(lanes), max_points, 2))  # Suponiendo puntos de carril (x, y)\n",
        "        for i, lane in enumerate(lanes):\n",
        "            num_points = min(len(lane), max_points)\n",
        "            padded_lanes[i, :num_points, 0] = lane[:num_points, 0]\n",
        "            padded_lanes[i, :num_points, 1] = lane[:num_points, 1]\n",
        "        return padded_lanes\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      batch_image_files = self.image_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "      batch_annotations = self.annotations[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
        "\n",
        "      images = np.array([self.preprocess_image(f) for f in batch_image_files])\n",
        "      lanes = []\n",
        "      for ann in batch_annotations:\n",
        "          lane_points = self.extract_lane_points(ann)\n",
        "          lane_array = np.array(lane_points)  # Convert to array numpy if not lo es\n",
        "          lanes.append(lane_array)\n",
        "\n",
        "      if not isinstance(images, np.ndarray):\n",
        "          raise TypeError(f\"Images is not a numpy array: {images}\")\n",
        "      if not isinstance(lanes, list):\n",
        "          raise TypeError(f\"Lanes is not a list: {lanes}\")\n",
        "\n",
        "      images = np.array(images)\n",
        "      lanes = self.pad_lanes(lanes)  # Call the function pad_lanes with lanes as list of arrays numpy\n",
        "\n",
        "      return images, lanes\n",
        "\n",
        "# Crear el dataset\n",
        "dataset = LaneDataset(image_files, annotations)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SRMY6f4hHems"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_images, val_images, train_annotations, val_annotations = train_test_split(img_files, annotations, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = LaneDataset(train_images, train_annotations)\n",
        "val_dataset = LaneDataset(val_images, val_annotations)"
      ],
      "metadata": {
        "id": "fSJ8IpvqHad1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Modelo de entrenamiento base{ run: \"auto\" }\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Concatenate, BatchNormalization, Activation\n",
        "\n",
        "def LaneNet(input_shape=(256, 512, 3)):\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Encoder\n",
        "    conv1 = Conv2D(64, 3, padding='same')(inputs)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Activation('relu')(conv1)\n",
        "    conv1 = Conv2D(64, 3, padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Activation('relu')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "\n",
        "    conv2 = Conv2D(128, 3, padding='same')(pool1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Activation('relu')(conv2)\n",
        "    conv2 = Conv2D(128, 3, padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Activation('relu')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "\n",
        "    conv3 = Conv2D(256, 3, padding='same')(pool2)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Activation('relu')(conv3)\n",
        "    conv3 = Conv2D(256, 3, padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Activation('relu')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "\n",
        "    conv4 = Conv2D(512, 3, padding='same')(pool3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Activation('relu')(conv4)\n",
        "    conv4 = Conv2D(512, 3, padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Activation('relu')(conv4)\n",
        "\n",
        "    # Decoder\n",
        "    up5 = UpSampling2D(size=(2, 2))(conv4)\n",
        "    up5 = Concatenate()([up5, conv3])\n",
        "    conv5 = Conv2D(256, 3, padding='same')(up5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Activation('relu')(conv5)\n",
        "    conv5 = Conv2D(256, 3, padding='same')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Activation('relu')(conv5)\n",
        "\n",
        "    up6 = UpSampling2D(size=(2, 2))(conv5)\n",
        "    up6 = Concatenate()([up6, conv2])\n",
        "    conv6 = Conv2D(128, 3, padding='same')(up6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Activation('relu')(conv6)\n",
        "    conv6 = Conv2D(128, 3, padding='same')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Activation('relu')(conv6)\n",
        "\n",
        "    up7 = UpSampling2D(size=(2, 2))(conv6)\n",
        "    up7 = Concatenate()([up7, conv1])\n",
        "    conv7 = Conv2D(64, 3, padding='same')(up7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Activation('relu')(conv7)\n",
        "    conv7 = Conv2D(64, 3, padding='same')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Activation('relu')(conv7)\n",
        "\n",
        "    # Output layer\n",
        "    output = Conv2D(1, 1, activation='sigmoid')(conv7)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Crear y compilar el modelo\n",
        "model = LaneNet()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n"
      ],
      "metadata": {
        "id": "pn12A5X78NTO",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LaneNet()\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "KjYHnuXdFqyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_dataset, validation_data=val_dataset, verbose=3, epochs=20)"
      ],
      "metadata": {
        "id": "okuM3FP0zZAr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}